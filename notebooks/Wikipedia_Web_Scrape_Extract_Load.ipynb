{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: firecrawl in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (1.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from firecrawl) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from firecrawl) (1.1.0)\n",
      "Requirement already satisfied: websockets in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from firecrawl) (15.0.1)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from firecrawl) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from firecrawl) (2.11.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=2.10.3->firecrawl) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=2.10.3->firecrawl) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=2.10.3->firecrawl) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=2.10.3->firecrawl) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from requests->firecrawl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from requests->firecrawl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from requests->firecrawl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from requests->firecrawl) (2025.1.31)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymysql in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (1.1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages if not already installed\n",
    "!pip install firecrawl\n",
    "!pip install pymysql\n",
    "!pip install python-dotenv\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl import FirecrawlApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env (data base credentials)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Firecrawl API key from the environment variables\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "if FIRECRAWL_API_KEY is None:\n",
    "    raise Exception(\"FIRECRAWL_API_KEY not set in the .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Firecrawl app with API key\n",
    "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the Wikipedia page for the S&P 500 companies.\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Firecrawl scraping request configuration.\n",
    "# We use a JSON format and ask Firecrawl to extract key pieces of information.\n",
    "# The prompt should ask for the essential fields such as ticker, security name, GICS sector, and other details.\n",
    "scrape_config = {\n",
    "    'formats': ['json'],\n",
    "    'jsonOptions': {\n",
    "        'prompt': 'Extract the following details as JSON for each S&P 500 company: ticker, security, gics_sector, gics_sub_industry, headquarters, date_added, and cik (if available).'\n",
    "    },\n",
    "    'timeout': 60000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error executing Firecrawl for URL https://en.wikipedia.org/wiki/List_of_S%26P_500_companies: Request Timeout: Failed to scrape URL as the request timed out. Request timed out - No additional error details provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     scrape_result = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscrape_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\firecrawl\\firecrawl.py:196\u001b[39m, in \u001b[36mFirecrawlApp.scrape_url\u001b[39m\u001b[34m(self, url, params)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscrape URL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\firecrawl\\firecrawl.py:1115\u001b[39m, in \u001b[36mFirecrawlApp._handle_error\u001b[39m\u001b[34m(self, response, action)\u001b[39m\n\u001b[32m   1114\u001b[39m \u001b[38;5;66;03m# Raise an HTTPError with the custom message and attach the response\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m requests.exceptions.HTTPError(message, response=response)\n",
      "\u001b[31mHTTPError\u001b[39m: Request Timeout: Failed to scrape URL as the request timed out. Request timed out - No additional error details provided.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     scrape_result = app.scrape_url(url, scrape_config)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError executing Firecrawl for URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Error executing Firecrawl for URL https://en.wikipedia.org/wiki/List_of_S%26P_500_companies: Request Timeout: Failed to scrape URL as the request timed out. Request timed out - No additional error details provided."
     ]
    }
   ],
   "source": [
    "# Execute the scraping process using Firecrawl\n",
    "try:\n",
    "    scrape_result = app.scrape_url(url, scrape_config)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error executing Firecrawl for URL {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse out the JSON data.\n",
    "# The structure of scrape_result depends on Firecrawl's response. \n",
    "# Here we assume the desired data is under the key 'json' and then a key such as 'companies'.\n",
    "# It might be necessary to adjust based on Firecrawl's actual output.\n",
    "try:\n",
    "    # Assume the JSON result is contained in scrape_result['json']\n",
    "    data_json = scrape_result['json']\n",
    "    # If the JSON structure directly returns a list of companies, use that:\n",
    "    if isinstance(data_json, list):\n",
    "        companies_data = data_json\n",
    "    # Otherwise, try extracting a sub-key (update \"companies\" if needed)\n",
    "    else:\n",
    "        companies_data = data_json.get('companies')\n",
    "    if not companies_data:\n",
    "        raise Exception(\"No company data found in Firecrawl response.\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error parsing JSON data from Firecrawl response: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
