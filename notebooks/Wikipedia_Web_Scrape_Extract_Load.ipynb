{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting BeautifulSoup\n",
      "  Using cached BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [25 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"C:\\Users\\spenc\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\spenc\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\spenc\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return hook(config_settings)\n",
      "        File \u001b[35m\"C:\\Users\\spenc\\AppData\\Local\\Temp\\pip-build-env-h403ttdk\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m334\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\spenc\\AppData\\Local\\Temp\\pip-build-env-h403ttdk\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m304\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\spenc\\AppData\\Local\\Temp\\pip-build-env-h403ttdk\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m522\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\spenc\\AppData\\Local\\Temp\\pip-build-env-h403ttdk\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m320\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m3\u001b[0m\n",
      "          \"You're trying to run a very old release of Beautiful Soup under Python 3. This will not work.\"\u001b[1;31m<>\u001b[0m\"Please use Beautiful Soup 4, available through the pip package 'beautifulsoup4'.\"\n",
      "                                                                                                         \u001b[1;31m^^\u001b[0m\n",
      "      \u001b[1;35mSyntaxError\u001b[0m: \u001b[35minvalid syntax\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymysql in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (1.1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\spenc\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4->bs4) (4.13.1)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages if not already installed\n",
    "!pip install BeautifulSoup\n",
    "!pip install pymysql\n",
    "!pip install python-dotenv\n",
    "!pip install pandas\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env (data base credentials)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the Wikipedia page for the S&P 500 companies.\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an exception for HTTP errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table with id \"constituents\"\n",
    "table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
    "if table is None:\n",
    "    raise Exception(\"Could not find the 'constituents' table on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to extract headers using <thead> if available, otherwise use the first row of the table.\n",
    "thead = table.find(\"thead\")\n",
    "if thead:\n",
    "    headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "else:\n",
    "    # Fallback: Use the first row (assumed to be header) from the table\n",
    "    first_row = table.find(\"tr\")\n",
    "    if first_row:\n",
    "        headers = [cell.get_text(strip=True) for cell in first_row.find_all([\"th\", \"td\"])]\n",
    "    else:\n",
    "        raise Exception(\"No header row found in the table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all data rows (skip header row if it's within <tbody>)\n",
    "tbody = table.find(\"tbody\")\n",
    "if tbody:\n",
    "    data_rows = tbody.find_all(\"tr\")\n",
    "else:\n",
    "    # If there is no <tbody>, get all <tr> elements and skip the first row (headers)\n",
    "    data_rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "    rows = []\n",
    "for tr in data_rows:\n",
    "    # Get all cell values from the row (using <td> elements)\n",
    "    cells = tr.find_all(\"td\")\n",
    "    row = [cell.get_text(strip=True) for cell in cells]\n",
    "    if row:\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample scraped data from Wikipedia:\n",
      "  Symbol             Security              GICSSector  \\\n",
      "0    MMM                   3M             Industrials   \n",
      "1    AOS          A. O. Smith             Industrials   \n",
      "2    ABT  Abbott Laboratories             Health Care   \n",
      "3   ABBV               AbbVie             Health Care   \n",
      "4    ACN            Accenture  Information Technology   \n",
      "\n",
      "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
      "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
      "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
      "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
      "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
      "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
      "\n",
      "          CIK      Founded  \n",
      "0  0000066740         1902  \n",
      "1  0000091142         1916  \n",
      "2  0000001800         1888  \n",
      "3  0001551152  2013 (1888)  \n",
      "4  0001467373         1989  \n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the scraped data\n",
    "df_sp500 = pd.DataFrame(rows, columns=headers)\n",
    "print(\"Sample scraped data from Wikipedia:\")\n",
    "print(df_sp500.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a database connection using credentials from your .env file\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "if None in (DB_USER, DB_PASSWORD, DB_HOST, DB_PORT, DB_NAME):\n",
    "    raise Exception(\"One or more database credentials are not set in the .env file.\")\n",
    "\n",
    "# Create a SQLAlchemy engine for MySQL\n",
    "engine = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Wikipedia data loaded successfully into 'raw_wikipedia_sp500' table.\n"
     ]
    }
   ],
   "source": [
    "# Write the DataFrame to a MySQL table named 'raw_wikipedia_sp500'\n",
    "df_sp500.to_sql(name=\"raw_wikipedia_sp500\", con=engine, if_exists=\"replace\", index=False)\n",
    "print(\"Scraped Wikipedia data loaded successfully into 'raw_wikipedia_sp500' table.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
